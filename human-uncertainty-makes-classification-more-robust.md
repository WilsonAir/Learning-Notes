# Human uncertainty makes classification more robust

**Human uncertainty makes classification more robust**

## **摘要**

深度神经网络的分类性能已开始在接近完美水平上渐近渐近。但是，它们**没有在训练集外进行概括的能力以及对对抗攻击的鲁棒性**。在本文中，我们通过训练**可反映人类感知不确定性的完整标签分布来解决此问题**。我们首先介绍一个称为CIFAR10H的新基准数据集，其中包含CIFAR10测试集每个图像的完整人类标签分布。然后，我们表明，尽管当代的分类器本身无法表现出类似人的不确定性，但对我们的数据集进行**显式训练可以缩小这一差距**，对训练分布测试数据集之外的测试集合也有比较好的泛化能力，并在对抗性攻击下也具有鲁棒性。

## **1.简介**

在自然图像分类基准上，最先进的卷积神经网络（CNN）模型被认为等于或甚至超过了人类的表现，以“ top-1准确性”来衡量-最可能的标记之间的对应关系通过模型和“ground truth”标签来获得一组测试图像。随着准确性的提高逐渐接近完美水平\[11\]，人们越来越关注训练集之外的表现，尤其是对相关刺激的概括能力\[39\]和对抗性例子的鲁棒性\[29\]。相比之下，在这些任务上，CNN的表现往往很差，而人类的表现却继续不错。

为了解决这个问题，并为训练分类器提供更好的标准，我们提出了一个替代目标：不仅试图捕获最可能的标签，而且试图捕获标签上的全部分布。**分类中的错误与正确答案一样具有参考价值。**例如，将狗和猫混为一类的网络，可能会比将卡车与猫混为一类的网络更好地进行归纳（参见\[1\]）。确实，请考虑图1所示的示例，其中CNN可能是置信度过低，置信度过高或系统上不正确，但仍获得了完美的准确性评分。**捕获这种相似性结构是有效泛化的关键部分**\[19\]，并且是在构建用于现实应用的分类模型时（例如，无人驾驶汽车中的避障对象）的重要考虑因素。

预测标签的更完整分布需要首先测量这些分布。鉴于我们无法直接从世界上提取 ground-truth 感官相似性，因此人类分类行为自然是进行这种比较的候选者。确实，人们通常在物体的种类上缺乏共识，**而人为错误常常传达有关视觉世界结构的重要信息**\[31\]。除了补充训练范式之外，**从人类收集这些完整的标签分布以更好地模拟人类偏见**并**预测其错误本身**本身很有趣-例如，这次是帮助无人驾驶汽车推断附近人类驾驶员的行为。最后，尽管在缩放数据集中的图像数量\[18\]和调查标签噪声\[40，12，48\]方面有大量工作，但从增加（信息性）标签分布的丰富度来识别收益方面所做的工作很少用于图像分类任务。

为此，我们做出了以下贡献：

* 我们提出了一个新颖的软标签数据集，我们称之为CIFAR10H，其中包括超过50,000个众包的人类分类判断，包括整个10,000张图像CIFAR10测试集的完整标签分布。
* 我们证明，当使用这些软标签训练最新的CNN分类器时，与硬标签控件相比，它们可以更好地归纳为样本外数据集。
* 我们提出了一种适用于人类标签的性能基准评估模型，并表明使用替代标签分布训练的模型也不能近似人类不确定性。
* 我们表明，当训练CNN在此基准上表现良好时，它们对对抗攻击的抵抗力会明显增强。

综上所述，我们的结果支持对模型泛化行为进行更细粒度的评估，并证明了一种将人类感知相似性整合到训练分类器范式中的方法的潜在实用性。

![clipboard.png](.gitbook/assets/0%20%286%29.png)

_图1：人类和我们最好的传统训练CNN（Shake-Shake \[11\]）达成一致的CIFAR10图像，但他们在系统上与其他选择有所不同。_

## **2.相关工作**

### **分层分类**

使用**类混淆**或**层次结构**来提高分类准确性或鲁棒性的工作可以追溯到早期的工作，例如，Griffin和Perona \[14\]，Marszalek和Schmid \[34\]或Zweig和Weinshall \[53\]。使用类别标签层次结构可以实现例如**共享表示**\[47、9、22\]，**有效组合模型**\[23\]或**通过层次结构预测\[32、8\]来提高分类的准确性**。有时会提出基准，使用分层指标进行评估（例如，ILSVRC 2010和2011的分层错误率\[41\]）。总体而言，尽管占主导地位的范例集中在评估top-K准确性，而不是分析系统的错误，并且层次结构主要用于培训。我们认为是时候重新考虑这一点了。首先，现代的大规模开放世界复杂数据集不再保证不重叠的对象类\[26\]，这使得层次类混淆特别有意义。其次，现有方法在top-K精度上已变得非常出色，因此，有必要将其针对对抗性示例\[44、13、2\]或分布移位\[45、39\]的稳健性日益关注。在这项工作中，我们向我们展示了对图像分类中人类不确定性的概括的首次大规模评估。

### 知识蒸馏

可以手动构建\[6，3\]帮助识别的标签层次结构，其能够从语言知识库\[10，9\]导出或自动学习\[14，19\]学习到。我们的工作最接近前者（手动构建），尽管我们没有明确地构造类层次结构，而是依靠类之间的人为混淆来推断给定图像的类之间的关系。尽管源于人类的困惑，但我们的工作与\[19\]的知识提炼方法有些相似。在知识蒸馏中，这些标签由预先训练的分类模型中平滑的softmax概率提供。当将软标签与基本事实结合在一起时，就可以实现模型传递和压缩的一种形式，因为softmax概率带有关键信息。此过程的原理与我们自己的相似：网络（和人类）通过将有关相似性结构的重要信息提取到我们根据图像及其类别推断的分布中，获得了极大的鲁棒性。但是，使用网络为他们提供知识（即知识提炼的标准应用）本身就存在问题，而无法与黄金标准进行比较：无法保证模型学到的相似性结构是正确的。

### 软标签

我们工作的核心贡献之一是使用由人为混乱而提供的软标签，代替 One-hot 标签编码。已经提出了几种方法来替代 One-hot 编码，例如，在大规模的1000+种分类中使用启发式方法平滑top-1标签\[43\]，或将测试时的人为不确定性纳入协作式计算机视觉系统中\[4\]。 mixup \[51\]是另一种近期开发的方法，用于基于示例对及其硬标签对的凸组合自动生成软标签，并且已被证明可以提高泛化性和对抗性，同时减少内存消耗。但是，由于线性约束在所有类别对上都是恒定的，并且标签是One-hot，因此很难了解此类标签的完全衡量感知相似度的柔和度。

### 人工研究

最后，有许多研究还利用人类专家在诸如医学诊断系统之类的相关分类领域的训练标签上提供分布信息\[35，36\]。尽管这些研究提出的理论案例支持我们自己的观点，但它们并未提供用于评估其他分类模型的大规模测试平台。值得注意的是，人为不确定性标签通常无需明确收集，但会在数据收集过程中自动变为可用。众包工作的大部分集中在调和人类标签和减轻他们之间的分歧上（参见Kovashka等人\[25\]进行的调查）。**我们的方法建议利用这些人为分歧来提高模型的准确性和鲁棒性，并补充旨在利用人工标签中的“错误”的现有工作\[27\]。**

## **3.从标签到标签分布**

图像分类任务的标准做法是使用常见基准数据集（例如ILSVRC12 \[41\]和CIFAR10 \[28\]）中提供的 “ Ground-Truth ” 标签进行训练，其中每个图像的“真实”类别是通过**人类共识来确定的**（模式选择）或**数据库创建者**。尽管在许多情况下都是有用的简化，但我们认为将这种近似将偏差引入学习框架具有重要的分布意义。看到这一点，首先考虑以下训练期间的标准损失最小化目标：

![clipboard.png](.gitbook/assets/1%20%285%29.png)

其中，对于观察到的数据样本 $$(xi , yi)^n_{i=1}$$ ，具有参数θ的模型的损耗L最小。我们以这种方式训练模型的目标是很好地概括未见数据：未来在给定从相同分布基础数据中提取的图像 $$(x_j)^ m_{j=1}$$ ，使得未观察到的标签上的预期损失降至最低：

![](.gitbook/assets/image%20%283%29.png)

 当我们考虑该乘积中的第二项时，我们可以看到，在数据集构建过程中使用模态标签只会是一个最佳估计，前提是对于任何刺激x，除了人类的共识类别之外，每个c类别的基础数据分布 $$p(y|x)$$ 为零。相比之下，当我们考虑图1所示的网络和人的不一致时，我们可以看到确实存在这种假设违反了人为概率分配的情况。

 那么，我们如何才能更自然地近似 $$p(y|x)$$呢？对于某些问题，很容易从一组真实的数据$$p(y|x)$$ 中进行采样，但是对于图像分类，我们必须依靠人类作为黄金标准来提供对 $$p(y|x)$$的良好估计。如果我们期望人类图像标签分布 $$p_{hum}(y|x)$$ 可以更好地反映给定图像类别上的自然分布，则可以将其用作 $$p(y|x)$$ 的改进估计量。

 在 $$f_θ(x)$$ 是分布 $$p_θ(y|x)$$ 和 $$L(f, x, y) $$ 是负对数似然的情况下，预期的损失会减少到人类分布与分类器预测的分布之间的交叉熵：

![](.gitbook/assets/image%20%286%29.png)

## 4.数据集构建

虽然像ImageNet \[41\]，Places \[52\]或COCO \[33\]这样的大规模流行数据集似乎是最好的起点，但CIFAR10尤其具有一些独特且吸引人的特性。

* 首先，对于社区来说，数据集仍然具有足够的兴趣，正在其上开发最先进的图像分类器\[11，21\]。
*  其次，数据集足够小，可以让我们为整个图像测试集收集大量的人类数据。
* 第三，图像的低分辨率有助于产生人类反应的变化。具有不重叠对象类别的高分辨率图像的人为错误率非常低，很难从相对较少的响应中获得有意义的信号。
* 最后，CIFAR10包含许多接近类别边界的示例，与其他经过精心挑选的数据集相比，每个数据集都被选为类别的一个很好的示例。

我们的最终CIFAR10H行为数据集由10,000个CIFAR10图像测试子集中的511,400个人类分类决策组成（每个图像约50个判断）。

### 4.1 影像刺激

 我们对CIFAR10的测试子集中的所有10,000张32×32彩色图像进行了人工判断。其中包含以下10个类别中的每个类别的1,000张图像：飞机，汽车，鸟，猫，鹿，狗，青蛙，马，船和卡车。这使我们能够使用相同的测试图像评估在CIFAR10训练集上进行预训练的模型，但是根据标签上的不同分布，将在下一部分中进行详细介绍。

### 4.2 人的判断

 通过Amazon Mechanical Turk \[5\]，我们在刺激计划中收集了511,400种人类分类，据我们所知，这是迄今为止单项研究中最大的分类。在任务中，要求参与者通过单击尽可能快速且准确的（但没有时间限制）周围的10个标签之一来对每个图像进行分类。候选人之间的标签位置不符。在初始训练阶段之后，每个参与者（总计2,571个）对200张图像进行了分类，每个类别中有20张图像。每20项试验，都会出现一张明显的图像作为注意力检查，并且得分低于75％的参与者将从最终分析中删除（共14项）。我们平均每张图片收集了51条判断（范围： $$47-63$$ ）。平均完成时间为15分钟，并且向工人支付了$1.50的报酬。图1显示了针对图像选择的分类判断分布示例。

## 5.分布转移下的一般化

我们的总体策略是使用我们的软标签来训练一系列分类器，并随着分布偏移的增加，在保留的验证集和大量归纳数据集上评估其性能。我们希望，当测试数据集日益缺乏分布时，有关图像标签不确定性的人工信息将最为有用。

### 5.1 设定

#### 模型

* 我们训练了八种CNN架构（VGG \[42\]，ResNet \[16\]，Wide ResNet \[50\]，ResNet preact \[17\]，ResNext \[49\]，DenseNet \[20\]，PyramidNet \[15\]和ShakeShake \[11\]）来最小化softmax输出和CIFAR10H中图像的完整人类标签分布之间的交叉熵损失。
* 我们使用PyTorch \[38\]训练了模型，并改编了脚注1中的资料库。
* 对于每种架构，我们使用10倍交叉验证（每次使用9,000张图像进行训练）训练10个模型，并在测试时对10个模型的平均结果运行。
* 为了获得更稳定的结果，我们使用k折而不是单个验证集。
* 为了可重复性，我们将所有模型的存储库中的默认超参数都用于\[39\]，以提高可重复性，但学习率除外。
* 我们使用Adam \[24\]优化器对每个模型进行了最多150个时期的训练，并针对基本学习率0.2、0.1、0.01和0.001（我们发现在所有情况下均为0.1）进行了网格搜索。

#### 测试数据集

第3节的关键预测是，**当推广到训练样本分布越来越多的情况下，我们标签中的不确定性将越来越有用**。通过检查以下数据集的泛化能力，我们通过经验测试了此预测：

**CIFAR10**：这是标准的数据集内评估。由于我们的CIFAR10H软标签用于CIFAR10测试集，因此在这里，我们使用标准CIFAR10训练集的50,000张图像来评估模型。

**CIFAR10.1v6，v4**：这是\[39\]创建的两个2,000图像近样本数据集，用于评估对CIFAR10经常用于验证的“测试”数据的拟合。这些图像取自TinyImages \[46\]，并且与CIFAR10中的子类分布相匹配。 v6每班有200张图像，而v4是原始班级不平衡版本（重叠90％）。

**CINIC10**：这是样本外通用测试。\[7\]收集的CINIC10数据集包含CIFAR10图像和来自等效类的重新缩放的ImageNet图像\[7\]。例如，来自airplane, aeroplane, plane \(airliner\) and airplane, aeroplane, plane \(bomber\)的ImageNet类的图像已分配给飞机CIFAR10顶级类。在这里，我们仅使用从ImageNet拍摄的210,000张图像。

**ImageNet-Far**：最后，作为更强的分布转移示例，我们构建了ImageNet-Far。如上所述，我们使用了重新缩放的ImageNet图像，但是选择了可能不在CIFAR10同义类的直接继承下的类。例如，对于CIFAR10标签鹿，我们包括了ImageNet类别的ibex，瞪羚，对于CIFAR10标签马来说，我们包含了ImageNet类别的斑马（CINIC10中未包括）。

#### 推广措施

我们根据**准确率**和**交叉熵**来评估每个测试集上的每个模型。**准确率**仍然是样本外概括任务分类性能的重要中心指标。由于**准确率**忽略了分配给猜测的概率，因此，我们还使用交叉熵度量来评估模型行为：它在最高预测中的置信度如何，以及其在替代类别上的分布是否合理。

> 请注意，当使用One-hot向量计算交叉熵时，这种解释自然会出现，因为只有分配给 Ground Truth 选择的概率质量才有助于得分。
>
> 当计算与One-hot向量不同的，分配质量的人类软标签时，交叉熵变得更加有用。

在这种情况下，网络的第二种猜测可能会给损失造成很大的影响，**而第二种猜测为图像提供了最容易混淆的类别**。为了提供对此更容易解释的启发式度量，我们引入了一种称为**次优准确性（SBA）的新准确性度量**。尽管top-1的准确性可能在很大程度上是渐近线，但我们希望SBA的收益仍有一段路要走。

![&#x56FE;2&#xFF1A;&#x6CDB;&#x5316;&#x7ED3;&#x679C;&#x3002;&#x5DE6;&#xFF1A;&#x76F8;&#x5BF9;&#x4E8E;&#x771F;&#x5B9E;&#x6807;&#x7B7E;&#x7684;&#x7CBE;&#x5EA6;&#xFF0C;&#x5BF9;&#x4E8E;&#x8D8A;&#x6765;&#x8D8A;&#x4E0D;&#x8BAD;&#x7EC3;&#x7684;&#x6837;&#x672C;&#x5206;&#x5E03;&#xFF0C;&#x5728;&#x6574;&#x4E2A;CNN&#x4E2D;&#x5E73;&#x5747;&#x3002;&#x4F7F;&#x7528;&#x4EBA;&#x5DE5;&#x6807;&#x7B7E;&#x6807;&#x6CE8;&#x6BCF;&#x4E2A;CNN&#x548C;&#x6570;&#x636E;&#x96C6;&#x7684;&#x51C6;&#x786E;&#x6027;&#x66F4;&#x9AD8;&#x3002;&#x4E2D;&#x5FC3;&#xFF1A;&#x76F8;&#x5BF9;&#x4E8E;&#x771F;&#x5B9E;&#x6807;&#x7B7E;&#x7684;&#x4EA4;&#x53C9;&#x71B5;&#xFF0C;&#x5728;&#x6574;&#x4E2A;CNN&#x4E2D;&#x5E73;&#x5747;&#x3002;&#x4F7F;&#x7528;&#x4EBA;&#x5DE5;&#x6807;&#x7B7E;&#x4E3A;&#x6BCF;&#x4E2A;&#x5355;&#x72EC;&#x7684;CNN&#x548C;&#x6570;&#x636E;&#x96C6;&#x5E26;&#x6765;&#x7684;&#x635F;&#x5931;&#x66F4;&#x4F4E;&#x3002;&#x53F3;&#x56FE;&#xFF1A;&#x6240;&#x6709;&#x4F7F;&#x7528;CIFAR10H&#x4F38;&#x51FA;&#x96C6;&#x7684;&#x6A21;&#x578B;&#x7684;&#x7B2C;&#x4E8C;&#x6700;&#x4F73;&#x7CBE;&#x5EA6;&#xFF08;SBA&#xFF09;&#xFF0C;&#x5E73;&#x5747;&#x6298;&#x53E0;&#x500D;&#x6570;&#x3002;](.gitbook/assets/image%20%2811%29.png)

![&#x56FE;3&#xFF1A;&#xFF08;A&#xFF09;&#x7ECF;&#x8FC7;&#x786C;/&#x8F6F;&#x6807;&#x7B7E;&#x8BAD;&#x7EC3;&#x540E;&#x6B63;&#x786E;/&#x9519;&#x8BEF;&#x5206;&#x7C7B;&#x7684;&#x793A;&#x4F8B;&#x7684;&#x5E73;&#x5747;&#x4FE1;&#x5EA6;&#x3002;&#x8F6F;&#x6807;&#x7B7E;&#x6A21;&#x578B;&#x5728;&#x9519;&#x8BEF;&#x65F6;&#x7684;&#x786E;&#x5B9A;&#x6027;&#x8FDC;&#x4E0D;&#x5982;&#x786C;&#x6807;&#x7B7E;&#x63A7;&#x4EF6;&#xFF0C;&#x5728;&#x6B63;&#x786E;&#x65F6;&#x7684;&#x786E;&#x5B9A;&#x6027;&#x7A0D;&#x5DEE;&#x3002; &#xFF08;B&#xFF09;&#x8F6F;&#x6807;&#x7B7E;&#x8BAD;&#x7EC3;&#x4EA7;&#x751F;&#x7684;&#x9884;&#x6D4B;&#x5C06;&#x6982;&#x7387;&#x8D28;&#x91CF;&#x5206;&#x914D;&#x5F97;&#x66F4;&#x50CF;&#x4EBA;&#xFF0C;&#x5E76;&#x4E14;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x9996;&#x9009;&#x3002;](.gitbook/assets/image.png)

### 5.2 人工标签可提高通用性

我们在 One-hot 标签（默认，对照）和CIFAR10H软人标签（我们的标签）上训练上述每个CNN，并以越来越多的样本外分布评估每个建议的测试集。

我们的第一个发现是，当我们在CIFAR10H软标签上训练CNN时，与我们的控件相比，**它们在所有通用数据集上的准确性都会提高（图2，左）**。对于每个单独的模型（未显示），该模式在各个交叉验证折叠中重复。泛化能力增强的一个关键特征是，随着测试数据集的训练分布越来越少（水平轴，从左到右），它会增加。例如，在CIFAR10上进行评估时，使用人为软标签只会给我们带来1％的改善（从83.5％到84.5％），而在ImageNet-Far上进行评估时，相同模型的平均准确率平均提高了2％（从49.4％提高到了2％） ％至51.4％）。

**当我们考虑交叉熵度量时，这种模式更加明显（图2，中心）**。例如，在CIFAR10上使用人类软标签可以使**交叉熵减少29％**（从0.7到0.5），而在ImageNetFar上进行评估时，相同模型的交叉熵平均可以减少38％（从2.9到1.8）。这些结果表明，在我们的软标签上训练的模型在其正确选择上显示出更好的一致性，并且在错误期间将更多的概率分配给真实情况。

最后，**在我们的软标签上训练的CNN与对照组相比，始终显示出SBA的显着提高，平均提高了5％**（图2，右）。这从广义上显示了泛化的改进：最有可能的两个类别的分布对于泛化的泛化退化具有重要的影响，我们希望一个好的模型能够为泛化降级，并且对于分类模型进行猜测时的性质也很重要。错误。

图3提供了超出整体泛化性能的验证行为上模型行为的另一幅图。令人鼓舞的是，我们发现，经过软标签训练的模型在错误时的置信度要比经过硬标签训练的控件低，但在正确时只有很少的置信度（图3a），并且更普遍地为人的不确定性模式提供了更好的拟合（图3b）。

## 6.替代软标签方法

上面，我们显示了样本外分类的好处来自对人类标签的训练。出现的一个自然问题是，这种改进是否是通过简单地使用软标签进行训练的结果（即允许模型将概率质量分布在一个以上的类上）还是由于这种分布明确地模仿了人类的不确定性而产生的。这里我们显示的答案是后者。

### 6.1 设定

**训练** 

我们着手证明，使用人类标签进行培训甚至在竞争基准上也能带来好处。我们使用与5.1节中相同的CNN架构和设置，但有一个明显的例外：我们在合并软标签之前对网络进行了预训练（这使我们能够尽可能地适应人类）。为此，我们使用标准CIFAR10训练协议进行训练，该协议使用50,000张图像和存储库中的最佳超参数进行训练，在很大程度上复制或超越了论文中针对每种体系结构提出的原始精度。然后，我们使用硬标签控件或CIFAR10测试集上的人类软标签对每个经过预训练的模型进行微调。这个微调阶段反映了5.1节中的训练短语：我们使用10倍，训练了150个时期，并搜索了0.1、0.01和0.001的学习率。

**评估**

我们在具有人类软标签和Ground-truth硬标签的CIFAR10H的保留褶皱上以及在CIFAR10.1v4和CIFAR10.1v6数据集的Ground-truth硬标签上评估保留结果。我们还将注意力转移到评估交叉熵而不是准确性上。使用CIFAR10预训练，所有模型的准确性都很高，但这并不能说明错误的置信度或“合理性”。另一方面，交叉熵的确做到了这一点：测量在硬标签上评估时的置信度，以及在人类软标签上评估时的错误的“合理性”。

### 6.2 方法

为了测试更简单且潜在同等有效的方法来逼近人类判断的不确定性，我们在下面提供了一些竞争基准。

**Ground-Truth控制**

我们考虑的第一个基准是“控制”微调条件，在这种条件下，我们使用相同的图像数据分割，但使用Ground-truth的硬标签进行微调。由于它利用了以前看不见的另外9,000张图像，因此有望在预训练模型上得到改善。

**类级处罚**

图像级人类软标签的一种简单得多的替代方法是**类别级软标签**。也就是说，与其指定每个图像与每个类别的相似程度，不如说是简单地使用类级别的惩罚来指定哪些类更容易混淆。但是，例如，尽管我们知道，狗和猫比狗和车平均更容易混淆，但尚不清楚最佳的类级罚款应该是多少。由于详尽搜索竞争性的阶级间罚分效率不高，因此我们建议通过对每个阶级内的人类概率进行求和并重新归一化（即产生正好10个唯一的软标签向量）来产生金标准的罚分。这也使我们能够确定人类软标签中的图像级信息是否真正被利用，而不是跨图像样本的类级统计信息。在此基线中，微调只是将这些压缩程度很大的软矢量用作目标。

**知识蒸馏**

如第2节所述，**经过训练的神经网络的softmax概率可以用作软标签**，因为它们包含由网络推断出的有关类别之间和图像之间相似性的信息。本节中的预训练网络提供了此类概率，因此提供了相应的基准。但是，我们可以从5.2节的结果中推断出，经过硬标签训练的CNN推断出的类概率与人类的近似，因为在人类方面引入明确的监督可以提供不同的结果。因此，为了在这方面提供更强的基准，**我们将所有八个模型的预测合并在一起（即，由于模型间变化的不确定性而提供了软预测）**。

 **混合** 

mixup是一种用于软标签生成的技术，可提高在CIFAR10上训练的自然图像分类模型的通用性\[51\]，请参见第2节。这样，它提供了一个有趣且具有竞争力的基准，可以将其与人类软标签进行比较。具体而言，混合通过对示例进行凸组合来生成软标签，从而鼓励它们之间的线性行为。这些组合构成了虚拟训练示例 $$(\overline{x},\overline{y})$$ ，这些示例是从附近的分布中取样的，并采用以下形式

![](.gitbook/assets/image%20%281%29.png)

 其中 $$(x_i,x_j)$$ 是数据集中的示例，而 $$(y_i,y_j)$$ 是它们的标签。插值 $$\lambda \in[0,1]$$ 的强度是根据Beta $$(\alpha, \alpha)$$ 进行采样的，其中α是超参数。对于我们的混合基准，我们将此程序应用于与上面使用的相同的10个拆分中的每个拆分相对应的地面真相标签。对于每种体系结构，我们以0.1为增量搜索从0.1到1.0的最佳α值。

 **软标签与采样**

最后，我们在上述软标签基准之外进行了另一项实验。第5节的结果表明，人类软标签很有用，但是我们如何最好地将它们纳入培训呢？在第3节中，我们以人为概率为目标进行了论证，以最大程度地减少预期损失。但是，另一个有效的选择是从 $$p_{hum}(y|x)$$ 进行采样，即从由以每个图像为条件的人类机率参数化的分类分布中采样一个热标签。如果我们每次将图像展示给网络以进行新的梯度更新时都对新标签进行采样，则标签不确定性仍会被合并，但是梯度中还会有其他变化，可以用作进一步的正则化。为了测试标签采样的任何此类优势，我们使用此方法微调了第二组相应的模型，为每个时期的每个图像采样了一个新标签。

![&#x8868;1&#xFF1A;&#x6BCF;&#x4E2A;&#x4FDD;&#x7559;&#x96C6;&#x7684;&#x4EA4;&#x53C9;&#x71B5;&#xFF08;&#x4ECE;&#x5DE6;&#x5230;&#x53F3;&#x7684;&#x5217;&#xFF1A;&#x4FDD;&#x7559;&#x4EBA;&#x8F6F;&#x6807;&#x7B7E;&#xFF08;c10H&#xFF09;&#xFF0C;&#x4FDD;&#x7559;&#x5730;&#x771F;&#x76F8;&#x6807;&#x7B7E;&#xFF08;c10&#xFF09;&#xFF0C;&#x6574;&#x4E2A;CIFAR10.1v4&#x6570;&#x636E;&#x96C6;&#x548C;&#x6574;&#x4E2A;CIFAR10.1v6&#x6570;&#x636E;&#x96C6;&#x3002;&#x7CBE;&#x7EC6;&#x8C03;&#x6574;&#xFF08;FT&#xFF09;&#x540E;&#xFF0C;&#x6211;&#x4EEC;&#x4EBA;&#x7C7B;&#x6807;&#x7B7E;&#x7684;&#x4EA4;&#x53C9;&#x71B5;&#x4F1A;&#x5927;&#x5927;&#x964D;&#x4F4E;&#xFF0C;&#x5C24;&#x5176;&#x662F;&#x5728;&#x4F7F;&#x7528;&#x4EBA;&#x7C7B;&#x76EE;&#x6807;&#x65F6;&#x3002;&#x5728;CIFAR10.1&#x7684;&#x4EA4;&#x53C9;&#x71B5;&#x65B9;&#x9762;&#xFF0C;&#x5BF9;&#x4EBA;&#x7C7B;&#x76EE;&#x6807;&#x7684;&#x5FAE;&#x8C03;&#x4E5F;&#x4F1A;&#x4EA7;&#x751F;&#x6700;&#x4F73;&#x7684;&#x6982;&#x62EC;&#x3002;](.gitbook/assets/image%20%2812%29.png)

### 6.3 人工软标签击败替代品

表1总结了每种体系结构和方法的结果。第一栏是我们对人的适应的主要量度。最后两个评估进一步概括。请注意，对于预训练模型（每个子表的第一行），与Ground-truth标签的交叉熵始终低于人类软标签，这证实了我们的期望：**人类软标签提供了其他信息，而且这些信息无法通过 Ground-truth 训练进行推断**。这是第一次测试发现，这些网络通常使用硬标签（即知识蒸馏）推断出的信息（信息概率）与人类不同。我们进一步测试了前八行中所有八个网络的整体（即，对人类软标签没有细微调整），尽管该模型比任何单独的经过硬标签训练的模型（交叉熵为0.41）更像人类，它仍然不能代替人工监督。标签的好处似乎也可以在概括过程中体现出来，因为在最后两列（即v4和v6保留集）中，它们的交叉熵比其他方法高。接下来，查看相同的最上面的几行，请注意，体系结构的新近度与适应人类的关系很少。实际上，Shake-Shake是八种最先进的技术，但在适应人类方面并不是三大模型之一。在每个子表的其余行中，我们可以看到使用各种微调方案对人的适应度增加了。在所有情况下，这都是可以预期的，因为与预训练的模型相比，所有这些模型最终都具有更多的数据。但是，并非所有的微调方法都同样有效。重要的是，当使用我们的图像级软标签或使用它们对硬标签进行采样（底部两行）时，最好是贴近人类（第二列）。有趣的是，类别软标签（第4行）也有效，但程度较小。**混合比单独使用ground truth标签更有效，但比使用人工信息的任何方法都无效。**最后，我们注意到，尽管为简洁起见被省略，但我们发现在任何使用人类标签的条件下使用人类标签时，准确性都没有损失。

## 7.对抗攻击的鲁棒性

因为我们的软标签包含与感知边界的结构有关的图像相似性结构的信息，所以我们可能期望在预测它们的过程中学习到的表示形式将更能抵抗对抗性攻击，尤其是在相似类别有益的情况下攻击目标。此外，对知识提炼的后续探索\[19，37\]已经证明，这种做法可以支持对抗性的鲁棒性。如果人类对知觉的判断相似 这种功能要优于CNN所推导的功能（以 $$p(y|x)$$ 的形式），我们希望将人类知识提炼成CNN至少会提高鲁棒性。

 **设定**

我们使用第6节中相同的预训练和精细调整（硬与软）模型。为了在每种训练方案之后测量鲁棒性，我们针对硬分类标签评估准确性和交叉熵（后者再次是对置信度和熵的更敏感的度量）。作为攻击方法，我们使用PyTorch的ederkit2评估了两种加性噪声攻击：快速梯度符号方法（FGSM）\[29\]和投影梯度下降（PGD）\[30\]。对于这两种方法，我们以1为增量探索了4到8的 $$\iota_\infty$$ 范围。由于我们发现结果没有显着差异，因此为了简洁起见，我们使用常数 $$\iota_\infty$$ 限制为4来报告所有攻击结果。

![&#x8868; 2&#xFF1A;&#x5BF9; CIFAR10 &#x5FAE;&#x8C03;&#xFF08;&#x57FA;&#x51C6;&#xFF09;&#x548C; CIFAR10H &#x5FAE;&#x8C03;&#x7F51;&#x7EDC;&#x8FDB;&#x884C; FGSM &#x653B;&#x51FB;&#x540E;&#x7684;&#x51C6;&#x786E;&#x6027;&#x548C;&#x4EA4;&#x53C9;&#x6027;&#x3002;&#x4F7F;&#x7528;&#x4EBA;&#x6807;&#x7B7E;&#x603B;&#x662F;&#x5BFC;&#x81F4;&#x8F83;&#x4F4E;&#xFF08;&#x66F4;&#x597D;&#x7684;&#xFF09;&#x4EA4;&#x53C9;&#x71B5;&#x635F;&#x5931;&#xFF0C;&#x5728;&#x5927;&#x591A;&#x6570;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6709;&#x7740;&#x66F4;&#x9AD8;&#x7684;&#x7CBE;&#x5EA6;&#x3002;](.gitbook/assets/image%20%289%29.png)

**人工软标签赋予鲁棒性** 

表2中报告了FGSM结果，在CIFAR10测试集中所有10,000张图像的平均值。在所有情况下，与使用原始的一键式标签进行细调相比，在攻击人为调整的网络后，交叉熵（攻击方法力图最大程度地降低）要低得多（大约一半）。对于八种架构中的五种，使用人类软目标时，准确性也会提高。最大的两个区别（宽Resnet和ResNet preact）也有利于人标签。请注意，除了以前使用人工标签进行的训练外，无需进行任何明确的（防御性）训练即可获得这些改进。如果没有积极的防御训练，那么经过足够的迭代，PGD的准确性就会达到0％。为了探索两种标签训练条件对PGD攻击的内在防御，我们在图4中绘制了每种架构和标签训练方案的损失增加。在使用标准标签进行训练时，每个网络的准确度都降低到0％，而在带有人工标签的网络中，准确度降低到1％，

![&#x56FE;4&#xFF1A;&#x4EA4;&#x53C9;&#x71B5;&#x4E0E;PGD&#x8FED;&#x4EE3;&#x7684;&#x5173;&#x7CFB;&#x3002;&#x8FDE;&#x7EED;&#x8FED;&#x4EE3;&#x4F1A;&#x6309;&#x9884;&#x671F;&#x589E;&#x52A0;&#x4EA4;&#x53C9;&#x71B5;&#xFF0C;&#x4F46;&#x5728;&#x8F6F;&#x6807;&#x7B7E;&#x7EC6;&#x8C03;&#x540E;&#xFF0C;&#x4EA4;&#x53C9;&#x71B5;&#x4F1A;&#x66F4;&#x7F13;&#x6162;&#x3002;](.gitbook/assets/image%20%282%29.png)

前者的损失更快地上升，而后者的渐近则渐近。简而言之，成功攻击行为更像人类的网络需要付出更大的努力。

## 8.讨论

在这项工作中，**我们证明了在图像级别上纳入有关人类类别不确定性的信息可以帮助防止分布转移和对抗攻击的危险**。值得注意的是，常见的分类基准通常自然不会自己提供这种保护\[45\]。此外，除了明确地合并此信息外，它还提供了一种方法来衡量我们的学习算法是否在推断良好的相似性结构（仅表现为top-1以外）。如果我们可以找到能够获得此类信息的良好学习程序，那么我们就可以在模型中获得类似人的鲁棒性，而无需明确的人为监督。但是，开发这样一个健壮的模型将花费大量时间和精力，即使不用于训练，我们的数据集也可以在衡量这一进展方面迈出第一步（相对于流行基准而言，这是最初的金标准）。

尽管我们的数据收集方法似乎并不能立即扩展到更大的训练集，但是肯定有可能以与我们通常在计算上花费的费用相当的方式收集信息标签分布，以发现更好的前1名的架构。有趣的是，我们发现人类不确定性的大部分集中在我们数据集中大约30％的图像上，这意味着可以采用直接且更有效的方法来挖掘这些信息量更大的标签。无论如何，我们看到了这类数据集的主要贡献，例如旨在用于更大数据集的算法的测试环境。





